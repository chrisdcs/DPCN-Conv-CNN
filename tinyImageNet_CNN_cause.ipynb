{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import kornia as K\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import utils\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from utils.data import ZCA_Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyImageNetTrainDataset(Dataset):\n",
    "    def __init__(self, root_dir, wnids_file, transform=None):\n",
    "        self.root_dir = root_dir  # Path to the \"train\" directory\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = self._load_class_mapping(wnids_file)\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Load image paths and their corresponding labels\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_class_mapping(self, wnids_file):\n",
    "        # Load class IDs from wnids.txt and map to indices\n",
    "        with open(wnids_file, 'r') as f:\n",
    "            wnids = f.read().splitlines()\n",
    "        return {wnid: idx for idx, wnid in enumerate(wnids)}\n",
    "\n",
    "    def _load_data(self):\n",
    "        # Iterate over each class folder to load images and labels\n",
    "        for class_name in self.class_to_idx.keys():\n",
    "            class_folder = os.path.join(self.root_dir, class_name, \"images\")\n",
    "            # Check if the class folder exists\n",
    "            if os.path.isdir(class_folder):\n",
    "                # Get all JPEG images in the class folder\n",
    "                for filename in os.listdir(class_folder):\n",
    "                    if filename.endswith('.JPEG'):\n",
    "                        self.images.append(os.path.join(class_folder, filename))\n",
    "                        self.labels.append(self.class_to_idx[class_name])  # Append the index corresponding to the class\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  # Ensure images are in RGB format\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "def parse_val_annotations(annotation_file):\n",
    "    label_map = {}\n",
    "    with open(annotation_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            filename, label = parts[0], parts[1]\n",
    "            label_map[filename] = label\n",
    "    return label_map\n",
    "\n",
    "# Step 1: Load class IDs from wnids.txt to get a consistent mapping\n",
    "def load_class_mapping(wnids_file):\n",
    "    with open(wnids_file, 'r') as f:\n",
    "        wnids = [line.strip() for line in f]\n",
    "    class_to_idx = {wnid: idx for idx, wnid in enumerate(wnids)}\n",
    "    return class_to_idx\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Update the custom dataset to use the class mapping\n",
    "class TinyImageNetDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_file, wnids_file, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.label_map = parse_val_annotations(annotation_file)\n",
    "        self.class_to_idx = load_class_mapping(wnids_file)\n",
    "        \n",
    "        self.image_filenames = list(self.label_map.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_filenames[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Convert label name to index using the consistent class mapping\n",
    "        label_name = self.label_map[img_name]\n",
    "        label = self.class_to_idx[label_name]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "#labels = parse_val_annotations('data/tiny-imagenet-200-DPCN-u3/val/val_annotations.txt')\n",
    "#class_to_idx = load_class_mapping('data/tiny-imagenet-200-DPCN-u3/wnids.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 'u2'\n",
    "\n",
    "#traindir = f'data/tiny-imagenet-200-DPCN-MLCSC-L6/train'\n",
    "batch_size = 32\n",
    "#transform = transforms.Compose([\n",
    "    #transforms.Resize(256),  # Resize images to 256x256\n",
    "    #transforms.CenterCrop(224),  # Crop the center 224x224\n",
    "    #transforms.Resize((64, 64)),  # Resize images to 64x64\n",
    "#    transforms.ToTensor(),  # Convert PIL image to tensor\n",
    "#    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize with ImageNet stats\n",
    "#                         std=[0.229, 0.224, 0.225])\n",
    "#])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    #transforms.RandomResizedCrop(64, scale=(0.8, 1.0)),\n",
    "    #transforms.RandomHorizontalFlip(p=0.5),\n",
    "    #transforms.RandomRotation(degrees=15),\n",
    "    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    #transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "    transforms.ToTensor(), \n",
    "    # transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "#train_dataset = datasets.ImageFolder(\n",
    "#        traindir,\n",
    "#        transform = transform\n",
    "#    )\n",
    "\n",
    "#train_loader = torch.utils.data.DataLoader(\n",
    "#        train_dataset, batch_size=batch_size, shuffle=True,\n",
    "#        num_workers=8, pin_memory=True)\n",
    "\n",
    "\n",
    "wnids_file = f'data/tiny-imagenet-200-DPCN-{layer}/wnids.txt'\n",
    "annotation_file = f'data/tiny-imagenet-200-DPCN-{layer}/val/val_annotations.txt'\n",
    "testdir = f'data/tiny-imagenet-200-MLCSC-L6/val/images'\n",
    "traindir = f'data/tiny-imagenet-200-MLCSC-L6/train'\n",
    "\n",
    "\n",
    "train_dataset = TinyImageNetTrainDataset(traindir, wnids_file, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "\n",
    "test_dataset = TinyImageNetDataset(image_dir=testdir, annotation_file=annotation_file, wnids_file=wnids_file, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n02124075': 0, 'n04067472': 1, 'n04540053': 2, 'n04099969': 3, 'n07749582': 4, 'n01641577': 5, 'n02802426': 6, 'n09246464': 7, 'n07920052': 8, 'n03970156': 9, 'n03891332': 10, 'n02106662': 11, 'n03201208': 12, 'n02279972': 13, 'n02132136': 14, 'n04146614': 15, 'n07873807': 16, 'n02364673': 17, 'n04507155': 18, 'n03854065': 19, 'n03838899': 20, 'n03733131': 21, 'n01443537': 22, 'n07875152': 23, 'n03544143': 24, 'n09428293': 25, 'n03085013': 26, 'n02437312': 27, 'n07614500': 28, 'n03804744': 29, 'n04265275': 30, 'n02963159': 31, 'n02486410': 32, 'n01944390': 33, 'n09256479': 34, 'n02058221': 35, 'n04275548': 36, 'n02321529': 37, 'n02769748': 38, 'n02099712': 39, 'n07695742': 40, 'n02056570': 41, 'n02281406': 42, 'n01774750': 43, 'n02509815': 44, 'n03983396': 45, 'n07753592': 46, 'n04254777': 47, 'n02233338': 48, 'n04008634': 49, 'n02823428': 50, 'n02236044': 51, 'n03393912': 52, 'n07583066': 53, 'n04074963': 54, 'n01629819': 55, 'n09332890': 56, 'n02481823': 57, 'n03902125': 58, 'n03404251': 59, 'n09193705': 60, 'n03637318': 61, 'n04456115': 62, 'n02666196': 63, 'n03796401': 64, 'n02795169': 65, 'n02123045': 66, 'n01855672': 67, 'n01882714': 68, 'n02917067': 69, 'n02988304': 70, 'n04398044': 71, 'n02843684': 72, 'n02423022': 73, 'n02669723': 74, 'n04465501': 75, 'n02165456': 76, 'n03770439': 77, 'n02099601': 78, 'n04486054': 79, 'n02950826': 80, 'n03814639': 81, 'n04259630': 82, 'n03424325': 83, 'n02948072': 84, 'n03179701': 85, 'n03400231': 86, 'n02206856': 87, 'n03160309': 88, 'n01984695': 89, 'n03977966': 90, 'n03584254': 91, 'n04023962': 92, 'n02814860': 93, 'n01910747': 94, 'n04596742': 95, 'n03992509': 96, 'n04133789': 97, 'n03937543': 98, 'n02927161': 99, 'n01945685': 100, 'n02395406': 101, 'n02125311': 102, 'n03126707': 103, 'n04532106': 104, 'n02268443': 105, 'n02977058': 106, 'n07734744': 107, 'n03599486': 108, 'n04562935': 109, 'n03014705': 110, 'n04251144': 111, 'n04356056': 112, 'n02190166': 113, 'n03670208': 114, 'n02002724': 115, 'n02074367': 116, 'n04285008': 117, 'n04560804': 118, 'n04366367': 119, 'n02403003': 120, 'n07615774': 121, 'n04501370': 122, 'n03026506': 123, 'n02906734': 124, 'n01770393': 125, 'n04597913': 126, 'n03930313': 127, 'n04118538': 128, 'n04179913': 129, 'n04311004': 130, 'n02123394': 131, 'n04070727': 132, 'n02793495': 133, 'n02730930': 134, 'n02094433': 135, 'n04371430': 136, 'n04328186': 137, 'n03649909': 138, 'n04417672': 139, 'n03388043': 140, 'n01774384': 141, 'n02837789': 142, 'n07579787': 143, 'n04399382': 144, 'n02791270': 145, 'n03089624': 146, 'n02814533': 147, 'n04149813': 148, 'n07747607': 149, 'n03355925': 150, 'n01983481': 151, 'n04487081': 152, 'n03250847': 153, 'n03255030': 154, 'n02892201': 155, 'n02883205': 156, 'n03100240': 157, 'n02415577': 158, 'n02480495': 159, 'n01698640': 160, 'n01784675': 161, 'n04376876': 162, 'n03444034': 163, 'n01917289': 164, 'n01950731': 165, 'n03042490': 166, 'n07711569': 167, 'n04532670': 168, 'n03763968': 169, 'n07768694': 170, 'n02999410': 171, 'n03617480': 172, 'n06596364': 173, 'n01768244': 174, 'n02410509': 175, 'n03976657': 176, 'n01742172': 177, 'n03980874': 178, 'n02808440': 179, 'n02226429': 180, 'n02231487': 181, 'n02085620': 182, 'n01644900': 183, 'n02129165': 184, 'n02699494': 185, 'n03837869': 186, 'n02815834': 187, 'n07720875': 188, 'n02788148': 189, 'n02909870': 190, 'n03706229': 191, 'n07871810': 192, 'n03447447': 193, 'n02113799': 194, 'n12267677': 195, 'n03662601': 196, 'n02841315': 197, 'n07715103': 198, 'n02504458': 199}\n",
      "{'n02124075': 0, 'n04067472': 1, 'n04540053': 2, 'n04099969': 3, 'n07749582': 4, 'n01641577': 5, 'n02802426': 6, 'n09246464': 7, 'n07920052': 8, 'n03970156': 9, 'n03891332': 10, 'n02106662': 11, 'n03201208': 12, 'n02279972': 13, 'n02132136': 14, 'n04146614': 15, 'n07873807': 16, 'n02364673': 17, 'n04507155': 18, 'n03854065': 19, 'n03838899': 20, 'n03733131': 21, 'n01443537': 22, 'n07875152': 23, 'n03544143': 24, 'n09428293': 25, 'n03085013': 26, 'n02437312': 27, 'n07614500': 28, 'n03804744': 29, 'n04265275': 30, 'n02963159': 31, 'n02486410': 32, 'n01944390': 33, 'n09256479': 34, 'n02058221': 35, 'n04275548': 36, 'n02321529': 37, 'n02769748': 38, 'n02099712': 39, 'n07695742': 40, 'n02056570': 41, 'n02281406': 42, 'n01774750': 43, 'n02509815': 44, 'n03983396': 45, 'n07753592': 46, 'n04254777': 47, 'n02233338': 48, 'n04008634': 49, 'n02823428': 50, 'n02236044': 51, 'n03393912': 52, 'n07583066': 53, 'n04074963': 54, 'n01629819': 55, 'n09332890': 56, 'n02481823': 57, 'n03902125': 58, 'n03404251': 59, 'n09193705': 60, 'n03637318': 61, 'n04456115': 62, 'n02666196': 63, 'n03796401': 64, 'n02795169': 65, 'n02123045': 66, 'n01855672': 67, 'n01882714': 68, 'n02917067': 69, 'n02988304': 70, 'n04398044': 71, 'n02843684': 72, 'n02423022': 73, 'n02669723': 74, 'n04465501': 75, 'n02165456': 76, 'n03770439': 77, 'n02099601': 78, 'n04486054': 79, 'n02950826': 80, 'n03814639': 81, 'n04259630': 82, 'n03424325': 83, 'n02948072': 84, 'n03179701': 85, 'n03400231': 86, 'n02206856': 87, 'n03160309': 88, 'n01984695': 89, 'n03977966': 90, 'n03584254': 91, 'n04023962': 92, 'n02814860': 93, 'n01910747': 94, 'n04596742': 95, 'n03992509': 96, 'n04133789': 97, 'n03937543': 98, 'n02927161': 99, 'n01945685': 100, 'n02395406': 101, 'n02125311': 102, 'n03126707': 103, 'n04532106': 104, 'n02268443': 105, 'n02977058': 106, 'n07734744': 107, 'n03599486': 108, 'n04562935': 109, 'n03014705': 110, 'n04251144': 111, 'n04356056': 112, 'n02190166': 113, 'n03670208': 114, 'n02002724': 115, 'n02074367': 116, 'n04285008': 117, 'n04560804': 118, 'n04366367': 119, 'n02403003': 120, 'n07615774': 121, 'n04501370': 122, 'n03026506': 123, 'n02906734': 124, 'n01770393': 125, 'n04597913': 126, 'n03930313': 127, 'n04118538': 128, 'n04179913': 129, 'n04311004': 130, 'n02123394': 131, 'n04070727': 132, 'n02793495': 133, 'n02730930': 134, 'n02094433': 135, 'n04371430': 136, 'n04328186': 137, 'n03649909': 138, 'n04417672': 139, 'n03388043': 140, 'n01774384': 141, 'n02837789': 142, 'n07579787': 143, 'n04399382': 144, 'n02791270': 145, 'n03089624': 146, 'n02814533': 147, 'n04149813': 148, 'n07747607': 149, 'n03355925': 150, 'n01983481': 151, 'n04487081': 152, 'n03250847': 153, 'n03255030': 154, 'n02892201': 155, 'n02883205': 156, 'n03100240': 157, 'n02415577': 158, 'n02480495': 159, 'n01698640': 160, 'n01784675': 161, 'n04376876': 162, 'n03444034': 163, 'n01917289': 164, 'n01950731': 165, 'n03042490': 166, 'n07711569': 167, 'n04532670': 168, 'n03763968': 169, 'n07768694': 170, 'n02999410': 171, 'n03617480': 172, 'n06596364': 173, 'n01768244': 174, 'n02410509': 175, 'n03976657': 176, 'n01742172': 177, 'n03980874': 178, 'n02808440': 179, 'n02226429': 180, 'n02231487': 181, 'n02085620': 182, 'n01644900': 183, 'n02129165': 184, 'n02699494': 185, 'n03837869': 186, 'n02815834': 187, 'n07720875': 188, 'n02788148': 189, 'n02909870': 190, 'n03706229': 191, 'n07871810': 192, 'n03447447': 193, 'n02113799': 194, 'n12267677': 195, 'n03662601': 196, 'n02841315': 197, 'n07715103': 198, 'n02504458': 199}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.class_to_idx)\n",
    "print(test_dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define ResNet 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def conv_block(in_channels, out_channels, pool=False):\n",
    "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
    "              nn.BatchNorm2d(out_channels), \n",
    "              nn.ReLU(inplace=True)]\n",
    "    if pool: layers.append(nn.MaxPool2d(2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class ResNet9(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=200):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = conv_block(in_channels, 64)\n",
    "        self.conv2 = conv_block(64, 128, pool=True)\n",
    "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
    "        \n",
    "        self.conv3 = conv_block(128, 256, pool=True)\n",
    "        self.conv4 = conv_block(256, 512, pool=True)\n",
    "        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.MaxPool2d(4), \n",
    "                                        nn.Flatten(), \n",
    "                                        nn.Linear(2048, num_classes))\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.conv1(xb)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out) + out\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.res2(out) + out\n",
    "        out = self.classifier(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 6.106 accuracy: 3.125\n",
      "[1,   400] loss: 5.421 accuracy: 0.000\n",
      "[1,   600] loss: 5.198 accuracy: 0.000\n",
      "[1,   800] loss: 5.072 accuracy: 0.000\n",
      "[1,  1000] loss: 4.895 accuracy: 3.125\n",
      "[1,  1200] loss: 4.820 accuracy: 0.000\n",
      "[1,  1400] loss: 4.789 accuracy: 6.250\n",
      "[1,  1600] loss: 4.780 accuracy: 0.000\n",
      "[1,  1800] loss: 4.764 accuracy: 3.125\n",
      "[1,  2000] loss: 4.744 accuracy: 6.250\n",
      "[1,  2200] loss: 4.744 accuracy: 0.000\n",
      "[1,  2400] loss: 4.740 accuracy: 0.000\n",
      "[1,  2600] loss: 4.753 accuracy: 0.000\n",
      "[1,  2800] loss: 4.734 accuracy: 0.000\n",
      "[1,  3000] loss: 4.720 accuracy: 3.125\n",
      "Validation loss: 28.565 accuracy: 2.500\n",
      "\n",
      "[2,   200] loss: 4.817 accuracy: 3.125\n",
      "[2,   400] loss: 4.804 accuracy: 3.125\n",
      "[2,   600] loss: 4.773 accuracy: 0.000\n",
      "[2,   800] loss: 4.773 accuracy: 3.125\n",
      "[2,  1000] loss: 4.765 accuracy: 0.000\n",
      "[2,  1200] loss: 4.767 accuracy: 0.000\n",
      "[2,  1400] loss: 4.753 accuracy: 6.250\n",
      "[2,  1600] loss: 4.754 accuracy: 6.250\n",
      "[2,  1800] loss: 4.751 accuracy: 0.000\n",
      "[2,  2000] loss: 4.759 accuracy: 0.000\n",
      "[2,  2200] loss: 4.754 accuracy: 3.125\n",
      "[2,  2400] loss: 4.732 accuracy: 0.000\n",
      "[2,  2600] loss: 4.741 accuracy: 3.125\n",
      "[2,  2800] loss: 4.766 accuracy: 3.125\n",
      "[2,  3000] loss: 4.743 accuracy: 0.000\n",
      "Validation loss: 6.512 accuracy: 3.250\n",
      "\n",
      "[3,   200] loss: 4.731 accuracy: 6.250\n",
      "[3,   400] loss: 4.736 accuracy: 0.000\n",
      "[3,   600] loss: 4.738 accuracy: 3.125\n",
      "[3,   800] loss: 4.759 accuracy: 3.125\n",
      "[3,  1000] loss: 4.728 accuracy: 0.000\n",
      "[3,  1200] loss: 4.723 accuracy: 0.000\n",
      "[3,  1400] loss: 4.716 accuracy: 0.000\n",
      "[3,  1600] loss: 4.726 accuracy: 6.250\n",
      "[3,  1800] loss: 4.736 accuracy: 6.250\n",
      "[3,  2000] loss: 4.716 accuracy: 3.125\n",
      "[3,  2200] loss: 4.723 accuracy: 0.000\n",
      "[3,  2400] loss: 4.733 accuracy: 0.000\n",
      "[3,  2600] loss: 4.735 accuracy: 0.000\n",
      "[3,  2800] loss: 4.732 accuracy: 0.000\n",
      "[3,  3000] loss: 4.716 accuracy: 3.125\n",
      "Validation loss: 5.701 accuracy: 3.270\n",
      "\n",
      "[4,   200] loss: 4.718 accuracy: 3.125\n",
      "[4,   400] loss: 4.732 accuracy: 0.000\n",
      "[4,   600] loss: 4.707 accuracy: 0.000\n",
      "[4,   800] loss: 4.721 accuracy: 3.125\n",
      "[4,  1000] loss: 4.714 accuracy: 0.000\n",
      "[4,  1200] loss: 4.718 accuracy: 0.000\n",
      "[4,  1400] loss: 4.718 accuracy: 0.000\n",
      "[4,  1600] loss: 4.710 accuracy: 0.000\n",
      "[4,  1800] loss: 4.703 accuracy: 3.125\n",
      "[4,  2000] loss: 4.714 accuracy: 0.000\n",
      "[4,  2200] loss: 4.721 accuracy: 0.000\n",
      "[4,  2400] loss: 4.698 accuracy: 3.125\n",
      "[4,  2600] loss: 4.711 accuracy: 3.125\n",
      "[4,  2800] loss: 4.706 accuracy: 12.500\n",
      "[4,  3000] loss: 4.699 accuracy: 0.000\n",
      "Validation loss: 5.576 accuracy: 2.460\n",
      "\n",
      "[5,   200] loss: 4.711 accuracy: 0.000\n",
      "[5,   400] loss: 4.699 accuracy: 6.250\n",
      "[5,   600] loss: 4.690 accuracy: 3.125\n",
      "[5,   800] loss: 4.698 accuracy: 0.000\n",
      "[5,  1000] loss: 4.688 accuracy: 0.000\n",
      "[5,  1200] loss: 4.696 accuracy: 0.000\n",
      "[5,  1400] loss: 4.717 accuracy: 0.000\n",
      "[5,  1600] loss: 4.691 accuracy: 0.000\n",
      "[5,  1800] loss: 4.693 accuracy: 3.125\n",
      "[5,  2000] loss: 4.695 accuracy: 3.125\n",
      "[5,  2200] loss: 4.702 accuracy: 0.000\n",
      "[5,  2400] loss: 4.692 accuracy: 0.000\n",
      "[5,  2600] loss: 4.689 accuracy: 0.000\n",
      "[5,  2800] loss: 4.686 accuracy: 0.000\n",
      "[5,  3000] loss: 4.694 accuracy: 3.125\n",
      "Validation loss: 6.510 accuracy: 2.520\n",
      "\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Check if GPU is available\n",
    "\n",
    "net = ResNet9()\n",
    "net = net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "interval = 200\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        # calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total = labels.size(0)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        accuracy = correct / total\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % interval == interval-1:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / interval:.3f} accuracy: {accuracy*100:.3f}')\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    # Validation loss and accuracy\n",
    "    net.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            #_, predicted = torch.max(outputs.data, 1)\n",
    "            _, predicted = outputs.topk(5, 1, True, True)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.view(-1, 1)).sum().item()\n",
    "\n",
    "    print(f'Validation loss: {val_loss / len(test_loader):.3f} accuracy: {correct / total * 100:.3f}\\n')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the training images: 2.372 %\n"
     ]
    }
   ],
   "source": [
    "# test the model on training data\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "print(f'Accuracy of the network on the training images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 29.27 %\n"
     ]
    }
   ],
   "source": [
    "# test on test data\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "print(f'Accuracy of the network on the test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to \"/saved_models/resnet9_u3_tinyimagenet.pth\"\n",
    "#torch.save(net.state_dict(), \"saved_models/resnet9_u2_tinyimagenet.pth\")  # Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Using Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 'u2'\n",
    "\n",
    "traindir = f'data/tiny-imagenet-200-DPCN-{layer}/train'\n",
    "batch_size = 32\n",
    "#transform = transforms.Compose([\n",
    "    #transforms.Resize(256),  # Resize images to 256x256\n",
    "    #transforms.CenterCrop(224),  # Crop the center 224x224\n",
    "    #transforms.Resize((64, 64)),  # Resize images to 64x64\n",
    "#    transforms.ToTensor(),  # Convert PIL image to tensor\n",
    "#    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize with ImageNet stats\n",
    "#                         std=[0.229, 0.224, 0.225])\n",
    "#])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    #transforms.RandomResizedCrop(64, scale=(0.8, 1.0)),\n",
    "    #transforms.RandomHorizontalFlip(p=0.5),\n",
    "    #transforms.RandomRotation(degrees=15),\n",
    "    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    #transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.4802, 0.4481, 0.3975], [0.2302, 0.2265, 0.2262])\n",
    "    # transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "#train_dataset = datasets.ImageFolder(\n",
    "#        traindir,\n",
    "#        transform = transform\n",
    "#    )\n",
    "\n",
    "#train_loader = torch.utils.data.DataLoader(\n",
    "#        train_dataset, batch_size=batch_size, shuffle=True,\n",
    "#        num_workers=8, pin_memory=True)\n",
    "\n",
    "\n",
    "wnids_file = f'data/tiny-imagenet-200/wnids.txt'\n",
    "annotation_file = f'data/tiny-imagenet-200/val/val_annotations.txt'\n",
    "testdir = f'data/tiny-imagenet-200/val/images'\n",
    "traindir = f'data/tiny-imagenet-200/train'\n",
    "\n",
    "\n",
    "train_dataset = TinyImageNetTrainDataset(traindir, wnids_file, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "\n",
    "test_dataset = TinyImageNetDataset(image_dir=testdir, annotation_file=annotation_file, wnids_file=wnids_file, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 6.910 accuracy: 0.000\n",
      "[1,   400] loss: 5.638 accuracy: 0.000\n",
      "[1,   600] loss: 5.231 accuracy: 3.125\n",
      "[1,   800] loss: 4.912 accuracy: 6.250\n",
      "[1,  1000] loss: 4.736 accuracy: 9.375\n",
      "[1,  1200] loss: 4.615 accuracy: 3.125\n",
      "[1,  1400] loss: 4.462 accuracy: 3.125\n",
      "[1,  1600] loss: 4.365 accuracy: 9.375\n",
      "[1,  1800] loss: 4.192 accuracy: 15.625\n",
      "[1,  2000] loss: 4.115 accuracy: 9.375\n",
      "[1,  2200] loss: 4.008 accuracy: 25.000\n",
      "[1,  2400] loss: 3.925 accuracy: 15.625\n",
      "[1,  2600] loss: 3.828 accuracy: 15.625\n",
      "[1,  2800] loss: 3.708 accuracy: 15.625\n",
      "[1,  3000] loss: 3.680 accuracy: 12.500\n",
      "Validation loss: 3.569 accuracy: 45.710\n",
      "\n",
      "[2,   200] loss: 3.629 accuracy: 21.875\n",
      "[2,   400] loss: 3.495 accuracy: 25.000\n",
      "[2,   600] loss: 3.371 accuracy: 25.000\n",
      "[2,   800] loss: 3.333 accuracy: 21.875\n",
      "[2,  1000] loss: 3.255 accuracy: 21.875\n",
      "[2,  1200] loss: 3.224 accuracy: 21.875\n",
      "[2,  1400] loss: 3.167 accuracy: 43.750\n",
      "[2,  1600] loss: 3.148 accuracy: 37.500\n",
      "[2,  1800] loss: 3.113 accuracy: 25.000\n",
      "[2,  2000] loss: 3.098 accuracy: 12.500\n",
      "[2,  2200] loss: 3.039 accuracy: 34.375\n",
      "[2,  2400] loss: 2.995 accuracy: 31.250\n",
      "[2,  2600] loss: 2.950 accuracy: 25.000\n",
      "[2,  2800] loss: 2.960 accuracy: 28.125\n",
      "[2,  3000] loss: 2.933 accuracy: 25.000\n",
      "Validation loss: 2.873 accuracy: 60.050\n",
      "\n",
      "[3,   200] loss: 2.574 accuracy: 34.375\n",
      "[3,   400] loss: 2.600 accuracy: 43.750\n",
      "[3,   600] loss: 2.586 accuracy: 31.250\n",
      "[3,   800] loss: 2.528 accuracy: 40.625\n",
      "[3,  1000] loss: 2.599 accuracy: 43.750\n",
      "[3,  1200] loss: 2.598 accuracy: 34.375\n",
      "[3,  1400] loss: 2.551 accuracy: 40.625\n",
      "[3,  1600] loss: 2.521 accuracy: 31.250\n",
      "[3,  1800] loss: 2.560 accuracy: 34.375\n",
      "[3,  2000] loss: 2.492 accuracy: 46.875\n",
      "[3,  2200] loss: 2.507 accuracy: 62.500\n",
      "[3,  2400] loss: 2.495 accuracy: 40.625\n",
      "[3,  2600] loss: 2.546 accuracy: 43.750\n",
      "[3,  2800] loss: 2.496 accuracy: 46.875\n",
      "[3,  3000] loss: 2.429 accuracy: 31.250\n",
      "Validation loss: 2.705 accuracy: 63.510\n",
      "\n",
      "[4,   200] loss: 2.001 accuracy: 62.500\n",
      "[4,   400] loss: 1.968 accuracy: 59.375\n",
      "[4,   600] loss: 2.011 accuracy: 34.375\n",
      "[4,   800] loss: 2.027 accuracy: 50.000\n",
      "[4,  1000] loss: 2.038 accuracy: 46.875\n",
      "[4,  1200] loss: 2.072 accuracy: 56.250\n",
      "[4,  1400] loss: 2.065 accuracy: 53.125\n",
      "[4,  1600] loss: 2.057 accuracy: 53.125\n",
      "[4,  1800] loss: 1.996 accuracy: 68.750\n",
      "[4,  2000] loss: 2.068 accuracy: 56.250\n",
      "[4,  2200] loss: 2.135 accuracy: 34.375\n",
      "[4,  2400] loss: 2.085 accuracy: 53.125\n",
      "[4,  2600] loss: 2.074 accuracy: 28.125\n",
      "[4,  2800] loss: 2.086 accuracy: 62.500\n",
      "[4,  3000] loss: 2.052 accuracy: 46.875\n",
      "Validation loss: 2.563 accuracy: 66.970\n",
      "\n",
      "[5,   200] loss: 1.477 accuracy: 68.750\n",
      "[5,   400] loss: 1.494 accuracy: 71.875\n",
      "[5,   600] loss: 1.511 accuracy: 75.000\n",
      "[5,   800] loss: 1.559 accuracy: 59.375\n",
      "[5,  1000] loss: 1.568 accuracy: 65.625\n",
      "[5,  1200] loss: 1.579 accuracy: 78.125\n",
      "[5,  1400] loss: 1.616 accuracy: 53.125\n",
      "[5,  1600] loss: 1.570 accuracy: 43.750\n",
      "[5,  1800] loss: 1.660 accuracy: 43.750\n",
      "[5,  2000] loss: 1.626 accuracy: 56.250\n",
      "[5,  2200] loss: 1.649 accuracy: 59.375\n",
      "[5,  2400] loss: 1.639 accuracy: 53.125\n",
      "[5,  2600] loss: 1.640 accuracy: 56.250\n",
      "[5,  2800] loss: 1.608 accuracy: 56.250\n",
      "[5,  3000] loss: 1.671 accuracy: 62.500\n",
      "Validation loss: 2.651 accuracy: 67.880\n",
      "\n",
      "[6,   200] loss: 0.956 accuracy: 84.375\n",
      "[6,   400] loss: 0.994 accuracy: 68.750\n",
      "[6,   600] loss: 1.024 accuracy: 78.125\n",
      "[6,   800] loss: 1.044 accuracy: 75.000\n",
      "[6,  1000] loss: 1.074 accuracy: 65.625\n",
      "[6,  1200] loss: 1.124 accuracy: 75.000\n",
      "[6,  1400] loss: 1.139 accuracy: 71.875\n",
      "[6,  1600] loss: 1.109 accuracy: 68.750\n",
      "[6,  1800] loss: 1.177 accuracy: 68.750\n",
      "[6,  2000] loss: 1.165 accuracy: 78.125\n",
      "[6,  2200] loss: 1.227 accuracy: 62.500\n",
      "[6,  2400] loss: 1.210 accuracy: 46.875\n",
      "[6,  2600] loss: 1.237 accuracy: 65.625\n",
      "[6,  2800] loss: 1.224 accuracy: 71.875\n",
      "[6,  3000] loss: 1.198 accuracy: 65.625\n",
      "Validation loss: 2.862 accuracy: 66.770\n",
      "\n",
      "[7,   200] loss: 0.554 accuracy: 93.750\n",
      "[7,   400] loss: 0.541 accuracy: 75.000\n",
      "[7,   600] loss: 0.600 accuracy: 78.125\n",
      "[7,   800] loss: 0.633 accuracy: 75.000\n",
      "[7,  1000] loss: 0.639 accuracy: 87.500\n",
      "[7,  1200] loss: 0.692 accuracy: 78.125\n",
      "[7,  1400] loss: 0.715 accuracy: 78.125\n",
      "[7,  1600] loss: 0.768 accuracy: 84.375\n",
      "[7,  1800] loss: 0.786 accuracy: 78.125\n",
      "[7,  2000] loss: 0.801 accuracy: 75.000\n",
      "[7,  2200] loss: 0.817 accuracy: 87.500\n",
      "[7,  2400] loss: 0.797 accuracy: 87.500\n",
      "[7,  2600] loss: 0.788 accuracy: 75.000\n",
      "[7,  2800] loss: 0.857 accuracy: 81.250\n",
      "[7,  3000] loss: 0.848 accuracy: 53.125\n",
      "Validation loss: 3.438 accuracy: 65.400\n",
      "\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Check if GPU is available\n",
    "\n",
    "net = ResNet9()\n",
    "net = net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "interval = 200\n",
    "for epoch in range(7):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        # calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total = labels.size(0)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        accuracy = correct / total\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % interval == interval-1:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / interval:.3f} accuracy: {accuracy*100:.3f}')\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    # Validation loss and accuracy\n",
    "    net.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            #_, predicted = torch.max(outputs.data, 1)\n",
    "            _, predicted = outputs.topk(5, 1, True, True)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.view(-1, 1)).sum().item()\n",
    "\n",
    "    print(f'Validation loss: {val_loss / len(test_loader):.3f} accuracy: {correct / total * 100:.3f}\\n')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the training images: 89 %\n"
     ]
    }
   ],
   "source": [
    "# test the model on training data\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "print(f'Accuracy of the network on the training images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 39 %\n"
     ]
    }
   ],
   "source": [
    "# test on test data\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "print(f'Accuracy of the network on the test images: {100 * correct / total} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "universe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
